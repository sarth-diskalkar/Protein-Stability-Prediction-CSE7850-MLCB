{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu35d-O8DLcm",
        "outputId": "24d92f4a-57de-439c-95b4-0d00113a92b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8148 entries, 0 to 8147\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   id        8148 non-null   int64  \n",
            " 1   sequence  8148 non-null   object \n",
            " 2   target    8148 non-null   float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 191.1+ KB\n",
            "None\n",
            "\n",
            "Training Data Description:\n",
            "                 id       target\n",
            "count   8148.000000  8148.000000\n",
            "mean    5033.456554    52.302745\n",
            "std     2924.877222     4.738780\n",
            "min        0.000000    38.520141\n",
            "25%     2500.750000    48.608375\n",
            "50%     5020.500000    51.620779\n",
            "75%     7555.750000    55.496006\n",
            "max    10091.000000    69.875198\n",
            "\n",
            "Testing Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1945 entries, 0 to 1944\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        1945 non-null   int64 \n",
            " 1   sequence  1945 non-null   object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 30.5+ KB\n",
            "None\n",
            "\n",
            "First 5 rows of Training Data:\n",
            "   id                                           sequence     target\n",
            "0   0  MESNHKSGDGLSGTQKEAALRALVQRTGYSLVQENGQRKYGGPPPG...  43.329788\n",
            "1   4  MCSLGLFPPPPPRGQVTLYEHNNELVTGSSYESPPPDFRGQWINLP...  51.782791\n",
            "2   5  MSKEERPGREEILECQVMWEPDSKKNTQMDRFRAAVGAACGLALES...  45.080222\n",
            "3   6  MRSSCVLLTALVALAAYYVYIPLPGSVSDPWKLMLLDATFRGAQQV...  59.651078\n",
            "4   7  MNYARFITAASAARNPSPIRTMTDILSRGPKSMISLAGGLPNPNMF...  55.985467\n",
            "\n",
            "First 5 rows of Testing Data:\n",
            "   id                                           sequence\n",
            "0   1  MALVFVYGTLKRGQPNHRVLRDGAHGSAAFRARGRTLEPYPLVIAG...\n",
            "1   2  MGKNKLLHPSLVLLLLVLLPTDASVSGKPQYMVLVPSLLHTETTEK...\n",
            "2   3  MWAQLLLGMLALSPAIAEELPNYLVTLPARLNFPSVQKVCLDLSPG...\n",
            "3   8  MAAGVPCALVTSCSSVFSGDQLVQHILGTEDLIVEVTSNDAVRFYP...\n",
            "4  11  MESESESGAAADTPPLETLSFHGDEEIIEVVELDPGPPDPDDLAQE...\n",
            "id          8148\n",
            "sequence    8148\n",
            "target      8148\n",
            "dtype: int64\n",
            "id          0\n",
            "sequence    0\n",
            "dtype: int64\n",
            "id          0\n",
            "sequence    0\n",
            "target      0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_path = '/content/drive/MyDrive/train.csv'\n",
        "test_path = '/content/drive/MyDrive/test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "print(\"Training Data Info:\")\n",
        "print(train_df.info())\n",
        "print(\"\\nTraining Data Description:\")\n",
        "print(train_df.describe())\n",
        "\n",
        "print(\"\\nTesting Data Info:\")\n",
        "print(test_df.info())\n",
        "\n",
        "print(\"\\nFirst 5 rows of Training Data:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nFirst 5 rows of Testing Data:\")\n",
        "print(test_df.head())\n",
        "#sum of samples that are not Nan/null in training data\n",
        "print(train_df.notna().sum())\n",
        "#sum of samples that are  Nan/null in test data\n",
        "print(test_df.isna().sum())\n",
        "#sum of samples that are  Nan/null in training data\n",
        "print(train_df.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "vocab = \"ARNDCQEGHILKMFPSTWYVU\"\n",
        "\n",
        "def one_hot_encode_seq(seq, vocab=vocab, max_length=None):\n",
        "    \"\"\" One-hot encode a protein sequence, padding to max_length. \"\"\"\n",
        "    if max_length is None:\n",
        "        max_length = len(seq)\n",
        "    encoding = np.zeros((max_length, len(vocab)), dtype=int)\n",
        "    aa_to_index = {aa: idx for idx, aa in enumerate(vocab)}\n",
        "    for i, aa in enumerate(seq[:max_length]):  # Slice sequence if longer than max_length\n",
        "        if aa in aa_to_index:\n",
        "            encoding[i, aa_to_index[aa]] = 1\n",
        "    return encoding.flatten()\n",
        "\n",
        "def frequency_encode_seq(seq, vocab=vocab):\n",
        "    \"\"\" Calculate frequency of each amino acid in a sequence. \"\"\"\n",
        "    encoding = np.zeros(len(vocab), dtype=float)\n",
        "    aa_to_index = {aa: idx for idx, aa in enumerate(vocab)}\n",
        "    seq_length = len(seq)\n",
        "    for aa in seq:\n",
        "        if aa in aa_to_index:\n",
        "            encoding[aa_to_index[aa]] += 1\n",
        "    if seq_length > 0:\n",
        "        encoding /= seq_length\n",
        "    return encoding\n",
        "\n",
        "def encode_dataset(df, encoding_func, max_length=None):\n",
        "    \"\"\" Apply encoding function to the 'sequence' column of a dataframe. \"\"\"\n",
        "    if 'max_length' in encoding_func.__code__.co_varnames:\n",
        "        encoded_features = np.array([encoding_func(seq, vocab, max_length) for seq in df['sequence']])\n",
        "    else:\n",
        "        encoded_features = np.array([encoding_func(seq, vocab) for seq in df['sequence']])\n",
        "    return encoded_features\n",
        "\n",
        "max_length = max(train_df['sequence'].str.len().max(), test_df['sequence'].str.len().max())\n",
        "\n",
        "train_features_one_hot = encode_dataset(train_df, one_hot_encode_seq, max_length)\n",
        "test_features_one_hot = encode_dataset(test_df, one_hot_encode_seq, max_length)\n",
        "\n",
        "train_features_freq = encode_dataset(train_df, frequency_encode_seq)\n",
        "test_features_freq = encode_dataset(test_df, frequency_encode_seq)\n",
        "\n",
        "train_targets = train_df['target'].values"
      ],
      "metadata": {
        "id": "gLEuabobfAs8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_one_hot = LinearRegression()\n",
        "model_one_hot.fit(train_features_one_hot, train_targets)\n",
        "train_predictions_one_hot = model_one_hot.predict(train_features_one_hot)\n",
        "\n",
        "test_predictions_one_hot = model_one_hot.predict(test_features_one_hot)\n",
        "\n",
        "model_freq = LinearRegression()\n",
        "model_freq.fit(train_features_freq, train_targets)\n",
        "train_predictions_freq = model_freq.predict(train_features_freq)\n",
        "\n",
        "rmse_one_hot = np.sqrt(mean_squared_error(train_targets, train_predictions_one_hot))\n",
        "\n",
        "rmse_freq = np.sqrt(mean_squared_error(train_targets, train_predictions_freq))\n",
        "\n",
        "print(\"RMSE with One-Hot Encoding:\", rmse_one_hot)\n",
        "print(\"RMSE with Frequency Encoding:\", rmse_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3I7crAmVb2E",
        "outputId": "1c1f7ddd-75d0-4d8e-f5e8-eabc2085a1db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE with One-Hot Encoding: 0.5663342573821032\n",
            "RMSE with Frequency Encoding: 4.34529999116271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'target': test_predictions_one_hot\n",
        "})\n",
        "\n",
        "\n",
        "submission_df.to_csv('/content/drive/MyDrive/linear_regression_prediction.csv', index=False)"
      ],
      "metadata": {
        "id": "UKVVtj0kMGGV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model_rf.fit(train_features_one_hot, train_targets)\n",
        "train_predictions_rf = model_rf.predict(train_features_one_hot)\n",
        "test_predictions_rf = model_rf.predict(test_features_one_hot)\n",
        "\n",
        "rmse_rf = np.sqrt(mean_squared_error(train_targets, train_predictions_rf))\n",
        "print(\"Training RMSE with RandomForest:\", rmse_rf)\n",
        "submission_df_rf = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'target': test_predictions_rf\n",
        "})\n",
        "submission_df_rf.to_csv('/content/drive/MyDrive/withoutPCA_prediction_rf.csv', index=False)\n",
        "print(\"RandomForest CSV file has been created and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "7iDl0Fh-RMSE",
        "outputId": "01e8799d-1eb4-4245-82b4-23f7c7ff44e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-032ae0d727b9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Assuming the data has already been loaded and encoded as before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_predictions_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_predictions_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "pca = PCA(n_components=200)\n",
        "train_features_pca = pca.fit_transform(train_features_one_hot)\n",
        "test_features_pca = pca.transform(test_features_one_hot)\n",
        "\n",
        "model_rf = RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42, n_jobs=-1)\n",
        "model_rf.fit(train_features_pca, train_targets)\n",
        "\n",
        "train_predictions_rf = model_rf.predict(train_features_pca)\n",
        "rmse_rf = np.sqrt(mean_squared_error(train_targets, train_predictions_rf))\n",
        "print(\"Training RMSE with RandomForest:\", rmse_rf)\n",
        "\n",
        "test_predictions_rf = model_rf.predict(test_features_pca)\n",
        "submission_df_rf = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'target': test_predictions_rf\n",
        "})\n",
        "submission_df_rf.to_csv('/content/drive/MyDrive/estimators50_PCAprediction_rf.csv', index=False)\n",
        "print(\"RandomForest CSV file has been created and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5ZMaPnklS4h",
        "outputId": "72868991-114a-4367-f717-5fd903f4d5e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE with RandomForest: 3.9445961902588302\n",
            "RandomForest CSV file has been created and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "pca = PCA(n_components=200)\n",
        "train_features_pca = pca.fit_transform(train_features_one_hot)\n",
        "test_features_pca = pca.transform(test_features_one_hot)\n",
        "\n",
        "model_rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
        "model_rf.fit(train_features_pca, train_targets)\n",
        "\n",
        "train_predictions_rf = model_rf.predict(train_features_pca)\n",
        "rmse_rf = np.sqrt(mean_squared_error(train_targets, train_predictions_rf))\n",
        "print(\"Training RMSE with RandomForest:\", rmse_rf)\n",
        "\n",
        "test_predictions_rf = model_rf.predict(test_features_pca)\n",
        "submission_df_rf = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'target': test_predictions_rf\n",
        "})\n",
        "submission_df_rf.to_csv('/content/drive/MyDrive/estimators100_PCAprediction_rf.csv', index=False)\n",
        "print(\"RandomForest CSV file has been created and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB7GILh-WKVo",
        "outputId": "1b2e4545-b65d-4d76-efff-001acd4c7c3b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE with RandomForest: 2.7864261661655534\n",
            "RandomForest CSV file has been created and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model_gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "model_gb.fit(train_features_one_hot, train_targets)\n",
        "\n",
        "train_predictions_gb = model_gb.predict(train_features_one_hot)\n",
        "\n",
        "test_predictions_gb = model_gb.predict(test_features_one_hot)\n",
        "\n",
        "rmse_gb = np.sqrt(mean_squared_error(train_targets, train_predictions_gb))\n",
        "print(\"Training RMSE with Gradient Boosting:\", rmse_gb)\n",
        "\n",
        "submission_df_gb = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'target': test_predictions_gb\n",
        "})\n",
        "\n",
        "submission_df_gb.to_csv('/content/drive/MyDrive/prediction_gb.csv', index=False)\n",
        "print(\"Gradient Boosting CSV file has been created and saved to '/content/drive/MyDrive/prediction_gb.csv'\")\n"
      ],
      "metadata": {
        "id": "u3sv7wH2TZf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_features_scaled = scaler.fit_transform(train_features_one_hot)\n",
        "test_features_scaled = scaler.transform(test_features_one_hot)\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(train_features_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  #no activation function\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(train_features_scaled, train_targets, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "train_predictions_nn = model.predict(train_features_scaled)\n",
        "rmse_nn = np.sqrt(mean_squared_error(train_targets, train_predictions_nn))\n",
        "print(\"Training RMSE with Neural Network:\", rmse_nn)\n",
        "\n",
        "test_predictions_nn = model.predict(test_features_scaled)\n",
        "\n",
        "submission_df_nn = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'target': test_predictions_nn.flatten()  #convert 2D array to 1D\n",
        "})\n",
        "submission_df_nn.to_csv('/content/drive/MyDrive/50epoch_prediction_nn.csv', index=False)\n",
        "print(\"Neural Network CSV file has been created and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbvYroDrXOB-",
        "outputId": "4e97ef8d-d005-42e2-b275-8f75d246bb63"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "204/204 [==============================] - 8s 25ms/step - loss: 3087.8518 - val_loss: 15102.3145\n",
            "Epoch 2/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4506.5898 - val_loss: 9698.2188\n",
            "Epoch 3/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 9883.3604 - val_loss: 126625.7188\n",
            "Epoch 4/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 19199.7676 - val_loss: 152216.1406\n",
            "Epoch 5/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6602.1489 - val_loss: 6555.6670\n",
            "Epoch 6/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 39114.5703 - val_loss: 98433.6016\n",
            "Epoch 7/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3049.8008 - val_loss: 93177.6875\n",
            "Epoch 8/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6938.5073 - val_loss: 68105.7969\n",
            "Epoch 9/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 437.9116 - val_loss: 75896.7344\n",
            "Epoch 10/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 615.6990 - val_loss: 75181.5703\n",
            "Epoch 11/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 205.6749 - val_loss: 82138.0234\n",
            "Epoch 12/50\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 501.7538 - val_loss: 81091.6797\n",
            "Epoch 13/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1139.8804 - val_loss: 68492.7031\n",
            "Epoch 14/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 148.2185 - val_loss: 59567.8516\n",
            "Epoch 15/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 62.7783 - val_loss: 61763.0039\n",
            "Epoch 16/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 37.2929 - val_loss: 58936.8242\n",
            "Epoch 17/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 42.1097 - val_loss: 57732.4062\n",
            "Epoch 18/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 31.2678 - val_loss: 58569.2070\n",
            "Epoch 19/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 37.9697 - val_loss: 55740.2891\n",
            "Epoch 20/50\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 89.2326 - val_loss: 53374.6211\n",
            "Epoch 21/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 117.3874 - val_loss: 43222.4648\n",
            "Epoch 22/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 178.5977 - val_loss: 45818.2734\n",
            "Epoch 23/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 131.1768 - val_loss: 31771.3457\n",
            "Epoch 24/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 223.7152 - val_loss: 27430.5156\n",
            "Epoch 25/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 280.7740 - val_loss: 12583.5781\n",
            "Epoch 26/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 273.0894 - val_loss: 10252.7617\n",
            "Epoch 27/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3524.3462 - val_loss: 6740.5933\n",
            "Epoch 28/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1142.1575 - val_loss: 625.8968\n",
            "Epoch 29/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1593.6542 - val_loss: 386.4504\n",
            "Epoch 30/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 10225.4541 - val_loss: 1060.3729\n",
            "Epoch 31/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 518.5807 - val_loss: 77.2131\n",
            "Epoch 32/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5102.9717 - val_loss: 2775.1750\n",
            "Epoch 33/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 914.0591 - val_loss: 78.0845\n",
            "Epoch 34/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 378.5583 - val_loss: 641.2761\n",
            "Epoch 35/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 173.7987 - val_loss: 326.7365\n",
            "Epoch 36/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 48.8332 - val_loss: 611.7101\n",
            "Epoch 37/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 18.9582 - val_loss: 525.3149\n",
            "Epoch 38/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 12.1483 - val_loss: 631.3832\n",
            "Epoch 39/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 10.2580 - val_loss: 391.3849\n",
            "Epoch 40/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.9869 - val_loss: 498.0486\n",
            "Epoch 41/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6.5803 - val_loss: 321.8349\n",
            "Epoch 42/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.2033 - val_loss: 401.5276\n",
            "Epoch 43/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.7248 - val_loss: 431.4090\n",
            "Epoch 44/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 10.7707 - val_loss: 266.3462\n",
            "Epoch 45/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 23.7344 - val_loss: 319.3981\n",
            "Epoch 46/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 36.2751 - val_loss: 106.7043\n",
            "Epoch 47/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 51.0361 - val_loss: 432.7897\n",
            "Epoch 48/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 99.8484 - val_loss: 155.7487\n",
            "Epoch 49/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 213.0905 - val_loss: 793.8447\n",
            "Epoch 50/50\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 154.1087 - val_loss: 925.9671\n",
            "255/255 [==============================] - 2s 7ms/step\n",
            "Training RMSE with Neural Network: 16.680073504236123\n",
            "61/61 [==============================] - 0s 7ms/step\n",
            "Neural Network CSV file has been created and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_features_scaled = scaler.fit_transform(train_features_one_hot)\n",
        "test_features_scaled = scaler.transform(test_features_one_hot)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(train_features_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "history = model.fit(train_features_scaled, train_targets, epochs=300, batch_size=32, validation_split=0.2)\n",
        "train_predictions_nn = model.predict(train_features_scaled)\n",
        "rmse_nn = np.sqrt(mean_squared_error(train_targets, train_predictions_nn))\n",
        "print(\"Training RMSE with Neural Network:\", rmse_nn)\n",
        "\n",
        "test_predictions_nn = model.predict(test_features_scaled)\n",
        "\n",
        "submission_df_nn = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'target': test_predictions_nn.flatten()\n",
        "})\n",
        "submission_df_nn.to_csv('/content/drive/MyDrive/300epoch_prediction_nn.csv', index=False)\n",
        "print(\"Neural Network CSV file has been created and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vX002PhcW8Y",
        "outputId": "24002b20-a941-4d9f-cfcf-4de38af07e52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "204/204 [==============================] - 6s 23ms/step - loss: 3304.5686 - val_loss: 739.4025\n",
            "Epoch 2/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1546.9404 - val_loss: 234.9787\n",
            "Epoch 3/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2656.3718 - val_loss: 450.9965\n",
            "Epoch 4/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4161.5210 - val_loss: 64.4959\n",
            "Epoch 5/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2381.0791 - val_loss: 78.7011\n",
            "Epoch 6/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 445.0100 - val_loss: 96.6510\n",
            "Epoch 7/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 266.8411 - val_loss: 537.8660\n",
            "Epoch 8/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 221.7504 - val_loss: 212.3617\n",
            "Epoch 9/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 34.2403 - val_loss: 65.8672\n",
            "Epoch 10/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 14.9821 - val_loss: 72.8078\n",
            "Epoch 11/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 11.4212 - val_loss: 55.6943\n",
            "Epoch 12/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.0321 - val_loss: 65.5305\n",
            "Epoch 13/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 7.9692 - val_loss: 67.0850\n",
            "Epoch 14/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.2762 - val_loss: 59.0042\n",
            "Epoch 15/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 10.1764 - val_loss: 46.3493\n",
            "Epoch 16/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 16.5805 - val_loss: 79.4432\n",
            "Epoch 17/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 15.1791 - val_loss: 64.3050\n",
            "Epoch 18/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.2105 - val_loss: 39.7354\n",
            "Epoch 19/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.2751 - val_loss: 44.4161\n",
            "Epoch 20/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 7.8498 - val_loss: 43.0240\n",
            "Epoch 21/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.6049 - val_loss: 34.6714\n",
            "Epoch 22/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6.5369 - val_loss: 38.4187\n",
            "Epoch 23/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6.7150 - val_loss: 39.0656\n",
            "Epoch 24/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6.4974 - val_loss: 60.5316\n",
            "Epoch 25/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.8708 - val_loss: 44.7638\n",
            "Epoch 26/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 32.6377 - val_loss: 213.9895\n",
            "Epoch 27/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 14.1235 - val_loss: 84.6228\n",
            "Epoch 28/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 11.1215 - val_loss: 227.9912\n",
            "Epoch 29/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 9.0739 - val_loss: 85.6071\n",
            "Epoch 30/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 10.1032 - val_loss: 159.4161\n",
            "Epoch 31/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 14.9760 - val_loss: 59.8875\n",
            "Epoch 32/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 16.3498 - val_loss: 202.9516\n",
            "Epoch 33/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 11.7088 - val_loss: 59.1776\n",
            "Epoch 34/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 10.8775 - val_loss: 173.2759\n",
            "Epoch 35/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6.6072 - val_loss: 169.7763\n",
            "Epoch 36/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.5856 - val_loss: 154.3112\n",
            "Epoch 37/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.8619 - val_loss: 137.8475\n",
            "Epoch 38/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.9059 - val_loss: 183.1280\n",
            "Epoch 39/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.8598 - val_loss: 146.5831\n",
            "Epoch 40/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.5344 - val_loss: 167.1341\n",
            "Epoch 41/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.7176 - val_loss: 114.8173\n",
            "Epoch 42/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.1799 - val_loss: 179.2607\n",
            "Epoch 43/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.9898 - val_loss: 87.9669\n",
            "Epoch 44/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.2793 - val_loss: 138.0350\n",
            "Epoch 45/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 108.0701 - val_loss: 40.1165\n",
            "Epoch 46/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 268.6698 - val_loss: 42.2475\n",
            "Epoch 47/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 20.4991 - val_loss: 37.9833\n",
            "Epoch 48/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 28.8843 - val_loss: 36.3796\n",
            "Epoch 49/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.5219 - val_loss: 50.9971\n",
            "Epoch 50/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 13.4976 - val_loss: 35.4006\n",
            "Epoch 51/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.5006 - val_loss: 39.9517\n",
            "Epoch 52/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.1614 - val_loss: 40.8513\n",
            "Epoch 53/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.5639 - val_loss: 42.5273\n",
            "Epoch 54/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.1458 - val_loss: 40.6707\n",
            "Epoch 55/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.1700 - val_loss: 42.5223\n",
            "Epoch 56/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.2771 - val_loss: 42.4236\n",
            "Epoch 57/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.4162 - val_loss: 39.3343\n",
            "Epoch 58/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.7142 - val_loss: 44.6778\n",
            "Epoch 59/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.1156 - val_loss: 47.7937\n",
            "Epoch 60/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.8106 - val_loss: 47.4468\n",
            "Epoch 61/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6.8815 - val_loss: 76.1983\n",
            "Epoch 62/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 13.4093 - val_loss: 32.1421\n",
            "Epoch 63/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.6820 - val_loss: 82.0266\n",
            "Epoch 64/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 6.9187 - val_loss: 40.3864\n",
            "Epoch 65/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.7150 - val_loss: 42.6262\n",
            "Epoch 66/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.7823 - val_loss: 48.0511\n",
            "Epoch 67/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.6205 - val_loss: 47.0911\n",
            "Epoch 68/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.6113 - val_loss: 50.9860\n",
            "Epoch 69/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.8300 - val_loss: 61.1134\n",
            "Epoch 70/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.9576 - val_loss: 53.0102\n",
            "Epoch 71/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.5402 - val_loss: 57.2627\n",
            "Epoch 72/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 9.1013 - val_loss: 50.8016\n",
            "Epoch 73/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.4462 - val_loss: 92.0886\n",
            "Epoch 74/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.1233 - val_loss: 104.3194\n",
            "Epoch 75/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.6864 - val_loss: 115.6299\n",
            "Epoch 76/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.6897 - val_loss: 112.5418\n",
            "Epoch 77/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.0391 - val_loss: 390.6969\n",
            "Epoch 78/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 7.7793 - val_loss: 125.4129\n",
            "Epoch 79/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 17.8561 - val_loss: 1866.3834\n",
            "Epoch 80/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 33.5964 - val_loss: 848.9002\n",
            "Epoch 81/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 11.1192 - val_loss: 47.5532\n",
            "Epoch 82/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.8134 - val_loss: 340.9467\n",
            "Epoch 83/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.3130 - val_loss: 75.5334\n",
            "Epoch 84/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.7530 - val_loss: 864.7055\n",
            "Epoch 85/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.1151 - val_loss: 450.2887\n",
            "Epoch 86/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.1662 - val_loss: 598.6009\n",
            "Epoch 87/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.8516 - val_loss: 301.1839\n",
            "Epoch 88/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.4388 - val_loss: 527.5148\n",
            "Epoch 89/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.4693 - val_loss: 430.3311\n",
            "Epoch 90/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.4996 - val_loss: 592.7020\n",
            "Epoch 91/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.7048 - val_loss: 549.7512\n",
            "Epoch 92/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.0278 - val_loss: 755.3813\n",
            "Epoch 93/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.7202 - val_loss: 563.6509\n",
            "Epoch 94/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.1695 - val_loss: 539.3890\n",
            "Epoch 95/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.1003 - val_loss: 576.0807\n",
            "Epoch 96/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.2585 - val_loss: 697.4506\n",
            "Epoch 97/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 11.4972 - val_loss: 465.6879\n",
            "Epoch 98/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 13.5345 - val_loss: 640.0887\n",
            "Epoch 99/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 10.2344 - val_loss: 39.4426\n",
            "Epoch 100/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.9534 - val_loss: 873.9115\n",
            "Epoch 101/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.2143 - val_loss: 691.6848\n",
            "Epoch 102/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.3945 - val_loss: 1443.0828\n",
            "Epoch 103/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.7270 - val_loss: 803.9429\n",
            "Epoch 104/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.7078 - val_loss: 559.8359\n",
            "Epoch 105/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.7835 - val_loss: 1021.0989\n",
            "Epoch 106/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.5447 - val_loss: 1068.9133\n",
            "Epoch 107/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.8361 - val_loss: 439.4873\n",
            "Epoch 108/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.9129 - val_loss: 617.7707\n",
            "Epoch 109/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.7767 - val_loss: 504.8694\n",
            "Epoch 110/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.7823 - val_loss: 1288.1080\n",
            "Epoch 111/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.9443 - val_loss: 89.1443\n",
            "Epoch 112/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.6916 - val_loss: 168.7102\n",
            "Epoch 113/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.5332 - val_loss: 112.3953\n",
            "Epoch 114/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.9984 - val_loss: 318.4176\n",
            "Epoch 115/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 2.0440 - val_loss: 403.2339\n",
            "Epoch 116/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.8095 - val_loss: 152.0241\n",
            "Epoch 117/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.5770 - val_loss: 470.6649\n",
            "Epoch 118/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.7121 - val_loss: 322.4459\n",
            "Epoch 119/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.6304 - val_loss: 145.3505\n",
            "Epoch 120/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 13.4069 - val_loss: 136.0305\n",
            "Epoch 121/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.5304 - val_loss: 114.2883\n",
            "Epoch 122/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.0330 - val_loss: 256.6530\n",
            "Epoch 123/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.5309 - val_loss: 395.3076\n",
            "Epoch 124/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.6961 - val_loss: 1309.4843\n",
            "Epoch 125/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 13.7757 - val_loss: 246.3723\n",
            "Epoch 126/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.8804 - val_loss: 326.3601\n",
            "Epoch 127/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 12.0802 - val_loss: 173.1970\n",
            "Epoch 128/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4918 - val_loss: 377.3757\n",
            "Epoch 129/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4918 - val_loss: 214.3518\n",
            "Epoch 130/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4526 - val_loss: 303.1111\n",
            "Epoch 131/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.3417 - val_loss: 259.1406\n",
            "Epoch 132/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.3115 - val_loss: 298.8547\n",
            "Epoch 133/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.3250 - val_loss: 282.2255\n",
            "Epoch 134/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4757 - val_loss: 235.2391\n",
            "Epoch 135/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4159 - val_loss: 279.0292\n",
            "Epoch 136/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4995 - val_loss: 163.4668\n",
            "Epoch 137/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.9365 - val_loss: 307.7944\n",
            "Epoch 138/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6.0651 - val_loss: 339.7371\n",
            "Epoch 139/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.6933 - val_loss: 545.1130\n",
            "Epoch 140/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.8197 - val_loss: 312.8232\n",
            "Epoch 141/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 1.4729 - val_loss: 343.3141\n",
            "Epoch 142/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4528 - val_loss: 953.9849\n",
            "Epoch 143/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 26.5064 - val_loss: 59.5232\n",
            "Epoch 144/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 28.4814 - val_loss: 32.4221\n",
            "Epoch 145/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 7.3158 - val_loss: 1664.7417\n",
            "Epoch 146/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 5.0465 - val_loss: 1484.6764\n",
            "Epoch 147/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 10.8972 - val_loss: 280.6336\n",
            "Epoch 148/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.2655 - val_loss: 2239.3474\n",
            "Epoch 149/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.1617 - val_loss: 495.8359\n",
            "Epoch 150/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.8407 - val_loss: 360.7274\n",
            "Epoch 151/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.3619 - val_loss: 651.5999\n",
            "Epoch 152/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.0785 - val_loss: 687.3867\n",
            "Epoch 153/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8703 - val_loss: 667.4194\n",
            "Epoch 154/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8526 - val_loss: 758.6699\n",
            "Epoch 155/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8383 - val_loss: 513.7177\n",
            "Epoch 156/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8721 - val_loss: 690.9781\n",
            "Epoch 157/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.0183 - val_loss: 376.4525\n",
            "Epoch 158/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.6591 - val_loss: 731.8138\n",
            "Epoch 159/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.5561 - val_loss: 459.2406\n",
            "Epoch 160/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.3226 - val_loss: 1399.5614\n",
            "Epoch 161/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.1881 - val_loss: 418.9839\n",
            "Epoch 162/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4920 - val_loss: 1181.1091\n",
            "Epoch 163/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.2462 - val_loss: 1171.9081\n",
            "Epoch 164/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.3211 - val_loss: 609.3179\n",
            "Epoch 165/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.3879 - val_loss: 164.8980\n",
            "Epoch 166/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4459 - val_loss: 359.5175\n",
            "Epoch 167/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.8414 - val_loss: 896.1140\n",
            "Epoch 168/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.8659 - val_loss: 47.2104\n",
            "Epoch 169/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.2641 - val_loss: 711.5277\n",
            "Epoch 170/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.3678 - val_loss: 482.0070\n",
            "Epoch 171/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 4.5079 - val_loss: 33.8062\n",
            "Epoch 172/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.7547 - val_loss: 176.7737\n",
            "Epoch 173/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6.6930 - val_loss: 73.1746\n",
            "Epoch 174/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.8250 - val_loss: 541.8951\n",
            "Epoch 175/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.0869 - val_loss: 274.7651\n",
            "Epoch 176/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.2455 - val_loss: 392.2140\n",
            "Epoch 177/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6579 - val_loss: 510.5913\n",
            "Epoch 178/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5635 - val_loss: 710.4051\n",
            "Epoch 179/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4344 - val_loss: 618.7708\n",
            "Epoch 180/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4796 - val_loss: 610.9589\n",
            "Epoch 181/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5237 - val_loss: 471.4572\n",
            "Epoch 182/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6533 - val_loss: 705.2063\n",
            "Epoch 183/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7848 - val_loss: 396.8992\n",
            "Epoch 184/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8347 - val_loss: 533.8857\n",
            "Epoch 185/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8632 - val_loss: 1108.9838\n",
            "Epoch 186/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4269 - val_loss: 185.4298\n",
            "Epoch 187/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.3668 - val_loss: 743.4250\n",
            "Epoch 188/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.5742 - val_loss: 88.5785\n",
            "Epoch 189/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.7448 - val_loss: 877.1185\n",
            "Epoch 190/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.8298 - val_loss: 423.0995\n",
            "Epoch 191/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.2457 - val_loss: 649.0080\n",
            "Epoch 192/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.7031 - val_loss: 912.6614\n",
            "Epoch 193/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.8050 - val_loss: 651.1101\n",
            "Epoch 194/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 1.3907 - val_loss: 745.8789\n",
            "Epoch 195/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.7410 - val_loss: 699.4012\n",
            "Epoch 196/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.7939 - val_loss: 753.8672\n",
            "Epoch 197/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 1.3241 - val_loss: 685.5414\n",
            "Epoch 198/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.1323 - val_loss: 2207.2910\n",
            "Epoch 199/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.3241 - val_loss: 1124.9585\n",
            "Epoch 200/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4265 - val_loss: 659.2927\n",
            "Epoch 201/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7365 - val_loss: 987.5864\n",
            "Epoch 202/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5556 - val_loss: 1026.4003\n",
            "Epoch 203/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4857 - val_loss: 888.9709\n",
            "Epoch 204/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4644 - val_loss: 859.5369\n",
            "Epoch 205/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5153 - val_loss: 1065.1532\n",
            "Epoch 206/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5843 - val_loss: 688.0391\n",
            "Epoch 207/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6848 - val_loss: 1001.0499\n",
            "Epoch 208/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7054 - val_loss: 478.5174\n",
            "Epoch 209/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6859 - val_loss: 805.6816\n",
            "Epoch 210/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.9725 - val_loss: 333.4489\n",
            "Epoch 211/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.9893 - val_loss: 673.8700\n",
            "Epoch 212/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.9163 - val_loss: 219.9699\n",
            "Epoch 213/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8512 - val_loss: 482.6616\n",
            "Epoch 214/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7768 - val_loss: 622.8594\n",
            "Epoch 215/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6052 - val_loss: 548.7841\n",
            "Epoch 216/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5821 - val_loss: 615.5995\n",
            "Epoch 217/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7979 - val_loss: 632.6095\n",
            "Epoch 218/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8154 - val_loss: 304.9936\n",
            "Epoch 219/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8775 - val_loss: 392.0069\n",
            "Epoch 220/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7804 - val_loss: 1006.3295\n",
            "Epoch 221/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.0167 - val_loss: 773.3473\n",
            "Epoch 222/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.9878 - val_loss: 348.2163\n",
            "Epoch 223/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7776 - val_loss: 266.8705\n",
            "Epoch 224/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8842 - val_loss: 414.9095\n",
            "Epoch 225/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.9504 - val_loss: 327.6199\n",
            "Epoch 226/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6208 - val_loss: 348.5270\n",
            "Epoch 227/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5660 - val_loss: 340.3989\n",
            "Epoch 228/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6239 - val_loss: 231.8811\n",
            "Epoch 229/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5440 - val_loss: 244.8193\n",
            "Epoch 230/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5481 - val_loss: 323.6957\n",
            "Epoch 231/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4951 - val_loss: 154.1520\n",
            "Epoch 232/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6000 - val_loss: 434.3546\n",
            "Epoch 233/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7012 - val_loss: 299.7236\n",
            "Epoch 234/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8803 - val_loss: 126.3802\n",
            "Epoch 235/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.0261 - val_loss: 160.8754\n",
            "Epoch 236/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.0951 - val_loss: 93.8937\n",
            "Epoch 237/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.9619 - val_loss: 41.8479\n",
            "Epoch 238/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 7.5915 - val_loss: 392.0928\n",
            "Epoch 239/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.0804 - val_loss: 43.0478\n",
            "Epoch 240/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.3867 - val_loss: 1197.0228\n",
            "Epoch 241/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.2514 - val_loss: 432.1676\n",
            "Epoch 242/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 163.9818 - val_loss: 6474.9302\n",
            "Epoch 243/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7828 - val_loss: 5790.7124\n",
            "Epoch 244/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.1502 - val_loss: 3028.3279\n",
            "Epoch 245/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8311 - val_loss: 11414.9795\n",
            "Epoch 246/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7329 - val_loss: 6658.4697\n",
            "Epoch 247/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5978 - val_loss: 6930.0474\n",
            "Epoch 248/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5948 - val_loss: 7741.7627\n",
            "Epoch 249/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6314 - val_loss: 11073.5889\n",
            "Epoch 250/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6492 - val_loss: 6702.7920\n",
            "Epoch 251/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5796 - val_loss: 11482.2393\n",
            "Epoch 252/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 0.7708 - val_loss: 8914.3369\n",
            "Epoch 253/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.7771 - val_loss: 13127.7783\n",
            "Epoch 254/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.0921 - val_loss: 19490.6934\n",
            "Epoch 255/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.2856 - val_loss: 4985.9922\n",
            "Epoch 256/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.9967 - val_loss: 23050.2129\n",
            "Epoch 257/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.9744 - val_loss: 16692.4844\n",
            "Epoch 258/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6054 - val_loss: 19261.0723\n",
            "Epoch 259/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4826 - val_loss: 20441.0898\n",
            "Epoch 260/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 0.4297 - val_loss: 17421.1875\n",
            "Epoch 261/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4453 - val_loss: 19767.3477\n",
            "Epoch 262/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4608 - val_loss: 17642.5957\n",
            "Epoch 263/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4919 - val_loss: 16273.6846\n",
            "Epoch 264/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6140 - val_loss: 17806.0176\n",
            "Epoch 265/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.8680 - val_loss: 19141.2559\n",
            "Epoch 266/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7881 - val_loss: 14330.5127\n",
            "Epoch 267/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.0607 - val_loss: 21223.5469\n",
            "Epoch 268/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 1.0369 - val_loss: 8333.4707\n",
            "Epoch 269/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.1376 - val_loss: 11920.7363\n",
            "Epoch 270/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6023 - val_loss: 9576.8545\n",
            "Epoch 271/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7781 - val_loss: 6149.3726\n",
            "Epoch 272/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.0957 - val_loss: 9118.5576\n",
            "Epoch 273/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 1.9268 - val_loss: 9043.7861\n",
            "Epoch 274/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.8264 - val_loss: 778.3137\n",
            "Epoch 275/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.9002 - val_loss: 1679.2859\n",
            "Epoch 276/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 14.6922 - val_loss: 3308.9988\n",
            "Epoch 277/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.6230 - val_loss: 449.8387\n",
            "Epoch 278/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.2466 - val_loss: 2245.5894\n",
            "Epoch 279/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6366 - val_loss: 3507.4023\n",
            "Epoch 280/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5654 - val_loss: 1557.0482\n",
            "Epoch 281/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5410 - val_loss: 3575.9905\n",
            "Epoch 282/300\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 0.4571 - val_loss: 2915.8647\n",
            "Epoch 283/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4342 - val_loss: 3750.5273\n",
            "Epoch 284/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4651 - val_loss: 3544.6604\n",
            "Epoch 285/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5263 - val_loss: 2117.0649\n",
            "Epoch 286/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5167 - val_loss: 4205.4180\n",
            "Epoch 287/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5108 - val_loss: 2502.3120\n",
            "Epoch 288/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5366 - val_loss: 5423.3423\n",
            "Epoch 289/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.6685 - val_loss: 2919.2878\n",
            "Epoch 290/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7093 - val_loss: 4709.5454\n",
            "Epoch 291/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.6030 - val_loss: 6317.3706\n",
            "Epoch 292/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4172 - val_loss: 494.5658\n",
            "Epoch 293/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.1974 - val_loss: 515.8749\n",
            "Epoch 294/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7465 - val_loss: 1264.9744\n",
            "Epoch 295/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.7043 - val_loss: 1096.9564\n",
            "Epoch 296/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5327 - val_loss: 907.0587\n",
            "Epoch 297/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4624 - val_loss: 872.8846\n",
            "Epoch 298/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4737 - val_loss: 739.5269\n",
            "Epoch 299/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.5058 - val_loss: 879.4896\n",
            "Epoch 300/300\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.4771 - val_loss: 884.5372\n",
            "255/255 [==============================] - 2s 7ms/step\n",
            "Training RMSE with Neural Network: 13.313846425019891\n",
            "61/61 [==============================] - 0s 7ms/step\n",
            "Neural Network CSV file has been created and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_features_scaled = scaler.fit_transform(train_features_one_hot)\n",
        "test_features_scaled = scaler.transform(test_features_one_hot)\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(train_features_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "history = model.fit(train_features_scaled, train_targets, epochs=125, batch_size=32, validation_split=0.2)\n",
        "train_predictions_nn = model.predict(train_features_scaled)\n",
        "rmse_nn = np.sqrt(mean_squared_error(train_targets, train_predictions_nn))\n",
        "print(\"Training RMSE with Neural Network:\", rmse_nn)\n",
        "test_predictions_nn = model.predict(test_features_scaled)\n",
        "\n",
        "submission_df_nn = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'target': test_predictions_nn.flatten()\n",
        "})\n",
        "submission_df_nn.to_csv('/content/drive/MyDrive/125_epoch_prediction_nn.csv', index=False)\n",
        "print(\"Neural Network CSV file has been created and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgaRfFvEdW_F",
        "outputId": "be3e921b-1290-44df-99f0-651c05a6aa33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/125\n",
            "204/204 [==============================] - 6s 22ms/step - loss: 2524.7468 - val_loss: 888.3648\n",
            "Epoch 2/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6005.3647 - val_loss: 4683.4810\n",
            "Epoch 3/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1624.9120 - val_loss: 4923.8491\n",
            "Epoch 4/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 7844.4644 - val_loss: 17601.9102\n",
            "Epoch 5/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 7287.2324 - val_loss: 21107.9395\n",
            "Epoch 6/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 11529.3701 - val_loss: 696.6773\n",
            "Epoch 7/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1406.9497 - val_loss: 2487.6296\n",
            "Epoch 8/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1094.0067 - val_loss: 2183.1260\n",
            "Epoch 9/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 133.2377 - val_loss: 1615.4587\n",
            "Epoch 10/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 173.5174 - val_loss: 1884.8842\n",
            "Epoch 11/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 89.4609 - val_loss: 1706.0592\n",
            "Epoch 12/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 45.5285 - val_loss: 2712.1555\n",
            "Epoch 13/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 110.7417 - val_loss: 2406.9648\n",
            "Epoch 14/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 138.2245 - val_loss: 674.9608\n",
            "Epoch 15/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 147.2064 - val_loss: 3788.1790\n",
            "Epoch 16/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 229.7628 - val_loss: 238.2841\n",
            "Epoch 17/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 245.4731 - val_loss: 4348.2090\n",
            "Epoch 18/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 165.4711 - val_loss: 37.3135\n",
            "Epoch 19/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 134.5076 - val_loss: 60.9265\n",
            "Epoch 20/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 131.2624 - val_loss: 41.2774\n",
            "Epoch 21/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 1048.5400 - val_loss: 708.7192\n",
            "Epoch 22/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 2121.5269 - val_loss: 1870.3730\n",
            "Epoch 23/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 167.1184 - val_loss: 73.9681\n",
            "Epoch 24/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 147.5429 - val_loss: 448.9543\n",
            "Epoch 25/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 75.2430 - val_loss: 79.4390\n",
            "Epoch 26/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 25.6492 - val_loss: 171.8668\n",
            "Epoch 27/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 137.4682 - val_loss: 835.3630\n",
            "Epoch 28/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 29.1755 - val_loss: 543.3517\n",
            "Epoch 29/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 143.5980 - val_loss: 353.6739\n",
            "Epoch 30/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 31.8052 - val_loss: 245.8239\n",
            "Epoch 31/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 33.0359 - val_loss: 387.5673\n",
            "Epoch 32/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 53.8577 - val_loss: 1004.6064\n",
            "Epoch 33/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 45.2563 - val_loss: 35.1029\n",
            "Epoch 34/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 200.1872 - val_loss: 315.0793\n",
            "Epoch 35/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 85.0843 - val_loss: 503.6826\n",
            "Epoch 36/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 160.1898 - val_loss: 743.3601\n",
            "Epoch 37/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 155.6315 - val_loss: 2866.8455\n",
            "Epoch 38/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 225.1270 - val_loss: 676.9418\n",
            "Epoch 39/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 545.5771 - val_loss: 120.5763\n",
            "Epoch 40/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 18.6860 - val_loss: 44.9470\n",
            "Epoch 41/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 14.8851 - val_loss: 87.1616\n",
            "Epoch 42/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 18.6013 - val_loss: 101.7921\n",
            "Epoch 43/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 49.8465 - val_loss: 553.5778\n",
            "Epoch 44/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2608.7493 - val_loss: 712.8085\n",
            "Epoch 45/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 280.8562 - val_loss: 752.7430\n",
            "Epoch 46/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1818.8785 - val_loss: 55.4642\n",
            "Epoch 47/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 193.8228 - val_loss: 43.3700\n",
            "Epoch 48/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 29.7569 - val_loss: 113.4104\n",
            "Epoch 49/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 17.5371 - val_loss: 37.3240\n",
            "Epoch 50/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 17.4826 - val_loss: 51.7574\n",
            "Epoch 51/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.8068 - val_loss: 50.9879\n",
            "Epoch 52/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.9474 - val_loss: 45.0175\n",
            "Epoch 53/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.0012 - val_loss: 49.2244\n",
            "Epoch 54/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.6745 - val_loss: 41.8071\n",
            "Epoch 55/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.7575 - val_loss: 56.0408\n",
            "Epoch 56/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.1718 - val_loss: 50.6515\n",
            "Epoch 57/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.1814 - val_loss: 62.4836\n",
            "Epoch 58/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.1291 - val_loss: 54.5439\n",
            "Epoch 59/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.2658 - val_loss: 84.6737\n",
            "Epoch 60/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.6299 - val_loss: 68.7155\n",
            "Epoch 61/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.3084 - val_loss: 152.4983\n",
            "Epoch 62/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 23.2202 - val_loss: 285.6991\n",
            "Epoch 63/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 25.0311 - val_loss: 250.2858\n",
            "Epoch 64/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 10.3986 - val_loss: 533.1554\n",
            "Epoch 65/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.3307 - val_loss: 615.5969\n",
            "Epoch 66/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 14.3376 - val_loss: 259.5426\n",
            "Epoch 67/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.9798 - val_loss: 947.6893\n",
            "Epoch 68/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.1850 - val_loss: 515.6584\n",
            "Epoch 69/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.8183 - val_loss: 485.1020\n",
            "Epoch 70/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.7466 - val_loss: 357.2699\n",
            "Epoch 71/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.8368 - val_loss: 516.9753\n",
            "Epoch 72/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.8721 - val_loss: 468.6933\n",
            "Epoch 73/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.0526 - val_loss: 791.6414\n",
            "Epoch 74/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.6780 - val_loss: 503.2108\n",
            "Epoch 75/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.7112 - val_loss: 505.8805\n",
            "Epoch 76/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 8.1474 - val_loss: 1662.4370\n",
            "Epoch 77/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 24.2015 - val_loss: 126.3612\n",
            "Epoch 78/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 41.4839 - val_loss: 699.3535\n",
            "Epoch 79/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 16.3934 - val_loss: 321.9075\n",
            "Epoch 80/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 20.0705 - val_loss: 1223.4121\n",
            "Epoch 81/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.8204 - val_loss: 593.5461\n",
            "Epoch 82/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 5.1246 - val_loss: 557.6382\n",
            "Epoch 83/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.5838 - val_loss: 633.5291\n",
            "Epoch 84/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.2153 - val_loss: 493.4678\n",
            "Epoch 85/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.8291 - val_loss: 423.6905\n",
            "Epoch 86/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 1.7138 - val_loss: 251.2915\n",
            "Epoch 87/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.7118 - val_loss: 673.9043\n",
            "Epoch 88/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.9956 - val_loss: 273.3287\n",
            "Epoch 89/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.5386 - val_loss: 484.3806\n",
            "Epoch 90/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.0418 - val_loss: 494.7498\n",
            "Epoch 91/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 2.9385 - val_loss: 445.0484\n",
            "Epoch 92/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.6451 - val_loss: 508.2528\n",
            "Epoch 93/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.1869 - val_loss: 433.7941\n",
            "Epoch 94/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.9345 - val_loss: 572.6479\n",
            "Epoch 95/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 6.4780 - val_loss: 472.6639\n",
            "Epoch 96/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.4806 - val_loss: 654.5826\n",
            "Epoch 97/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 14.3062 - val_loss: 699.7157\n",
            "Epoch 98/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.8344 - val_loss: 628.4280\n",
            "Epoch 99/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.9902 - val_loss: 701.1887\n",
            "Epoch 100/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4376 - val_loss: 727.0304\n",
            "Epoch 101/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.4274 - val_loss: 778.8912\n",
            "Epoch 102/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.6913 - val_loss: 570.3082\n",
            "Epoch 103/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.7198 - val_loss: 700.4811\n",
            "Epoch 104/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.0434 - val_loss: 635.1760\n",
            "Epoch 105/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 1.9267 - val_loss: 773.2972\n",
            "Epoch 106/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 85.3017 - val_loss: 493.8518\n",
            "Epoch 107/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 207.6145 - val_loss: 1668.4413\n",
            "Epoch 108/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 422.5742 - val_loss: 1177.9520\n",
            "Epoch 109/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 4.0168 - val_loss: 1635.4222\n",
            "Epoch 110/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 49.5867 - val_loss: 1649.2021\n",
            "Epoch 111/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 969.3346 - val_loss: 884.1402\n",
            "Epoch 112/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 33.5054 - val_loss: 894.0340\n",
            "Epoch 113/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 10.1761 - val_loss: 684.6989\n",
            "Epoch 114/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.8277 - val_loss: 935.1806\n",
            "Epoch 115/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 7.4137 - val_loss: 782.5515\n",
            "Epoch 116/125\n",
            "204/204 [==============================] - 2s 12ms/step - loss: 3.7002 - val_loss: 732.7314\n",
            "Epoch 117/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.5902 - val_loss: 817.3962\n",
            "Epoch 118/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.2382 - val_loss: 695.7823\n",
            "Epoch 119/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 0.9678 - val_loss: 786.9474\n",
            "Epoch 120/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 2.4760 - val_loss: 761.2090\n",
            "Epoch 121/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.8159 - val_loss: 664.9374\n",
            "Epoch 122/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.3484 - val_loss: 944.2421\n",
            "Epoch 123/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.3715 - val_loss: 839.5646\n",
            "Epoch 124/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 1.5798 - val_loss: 562.2961\n",
            "Epoch 125/125\n",
            "204/204 [==============================] - 2s 11ms/step - loss: 3.4582 - val_loss: 674.0817\n",
            "255/255 [==============================] - 2s 7ms/step\n",
            "Training RMSE with Neural Network: 11.839886253450803\n",
            "61/61 [==============================] - 0s 7ms/step\n",
            "Neural Network CSV file has been created and saved.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}